# ğŸ–¼ï¸ Image Filter Program

> **High-Resolution Real-Time Image Filtering and Convolution**  
> PNG â†’ PNG conversion with multiple filter options

---

## ğŸ“‹ Overview

This program reads an input PNG image, applies a selected filter, and saves the output as a new PNG file. It supports both **serial** and **parallel** execution modes for optimal performance.

---

## âœ¨ Supported Filters

| Filter | Description |
|--------|-------------|
| **sobel** | Edge detection filter |
| **gaussian** | Blur filter with configurable kernel size and sigma |
| **laplacian** | Edge enhancement filter |
| **sharpen** | Image sharpening filter |

---

## ğŸ“¦ Requirements
- `main_parallel.c`
- `main.c`
- `stb_image.h`
- `stb_image_write.h`
- GCC compiler

### Download STB Headers

Get the **stb** headers from:  
ğŸ”— [https://github.com/nothings/stb](https://github.com/nothings/stb)

---

## ğŸ”¨ Compilation

### Serial Version
```bash
gcc main.c -o image_filter_serial -lm
```

### Parallel Version
```bash
gcc-15 main_parallel.c -o image_filter_parallel -fopenmp -lm
```

## ğŸš€ Filter Usage 

### 1ï¸âƒ£ Sobel Edge Detection

Applies edge detection to create a grayscale edge-detected PNG.

**Command:**
```bash
# Serial
./image_filter_serial input.png output_sobel_serial.png sobel

# Parallel
./image_filter_parallel input.png output_sobel_parallel.png sobel
```

---

### 2ï¸âƒ£ Gaussian Blur

Applies blur effect with configurable parameters.

**Command:**
```bash
# Serial
./image_filter_serial input.png output_gaussian_serial.png gaussian 5 1.0

# Parallel
./image_filter_parallel input.png output_gaussian_parallel.png gaussian 5 1.0
```

**Parameters:**
- `ksize` â†’ Kernel size (odd integer: 3, 5, 7, â€¦)
- `sigma` â†’ Blur strength (e.g., 0.8, 1.0, 1.5, â€¦)

---

### 3ï¸âƒ£ Laplacian Edge Filter

Applies Laplacian edge enhancement.

**Command:**
```bash
# Serial
./image_filter_serial input.png output_laplacian_serial.png laplacian

# Parallel
./image_filter_parallel input.png output_laplacian_parallel.png laplacian
```

---

### 4ï¸âƒ£ Sharpen Filter

Enhances image sharpness.

**Command:**
```bash
# Serial
./image_filter_serial input.png output_sharpen_serial.png sharpen

# Parallel
./image_filter_parallel input.png output_sharpen_parallel.png sharpen
```

---

## âš¡ Performance Benchmarks


| Filter | Serial Execution Time |
|--------|----------------|
| Sobel | 0.016043s |
| Gaussian | 0.055719s |
| Laplacian | 0.053983s |
| Sharpen | 0.054240s |

---

## ğŸ“ General Usage Pattern

```bash
./image_filter_[serial|parallel] <input.png> <output.png> <filter> [parameters]
```

**Arguments:**
- `input.png` - Source image file
- `output.png` - Destination image file
- `filter` - Filter type (sobel, gaussian, laplacian, sharpen)
- `[parameters]` - Optional filter-specific parameters (e.g., ksize and sigma for gaussian)

---


# ğŸš€ OpenMP Parallelization Implementation Guide

> **Step-by-Step Guide to Parallelize Image Filter Program**  
> Transform serial code into high-performance parallel implementation

---

## ğŸ“‹ Table of Contents

- [Implementation Roadmap](#-implementation-roadmap)
- [Phase 1: Setup](#phase-1-setup-do-first)
- [Phase 2: Parallelize Functions](#phase-2-parallelize-functions-easiest--hardest)
- [Phase 3: Optimization](#phase-3-optimization-optional)
- [Testing Strategy](#-testing-strategy)
- [Common Mistakes](#ï¸-common-mistakes-to-avoid)
- [Implementation Order](#-implementation-order)

---

## ğŸ¯ Implementation Roadmap

### Progress Checklist

- [ ] **Phase 1:** Setup Thread Count Parameter
- [ ] **Phase 2:** Parallelize Functions
  - [ ] Step 2: `to_grayscale()`
  - [ ] Step 3: `sobel()`
  - [ ] Step 4: `build_gaussian()`
  - [ ] Step 5: `convolve_rgb()`
- [ ] **Phase 3:** Optimization with Scheduling

---

## Phase 1: SETUP (Do First)

### **STEP 1: Add Thread Count Parameter**

#### Changes Required:
1. **Modify `argc` check:** `< 4` â†’ `< 5`
2. **Add variable:** `int thread_count` in main
3. **Parse thread count:**
   ```c
   thread_count = strtol(argv[3], NULL, 10)
   ```
4. **Set OpenMP threads:**
   ```c
   omp_set_num_threads(thread_count)
   ```
5. **Shift argv indices:**
   - `mode` â†’ `argv[3]` becomes `argv[4]`
   - Gaussian `ksize` â†’ `argv[4]` becomes `argv[5]`
   - Gaussian `sigma` â†’ `argv[5]` becomes `argv[6]`
6. **Update Gaussian check:** `argc < 6` â†’ `argc < 7`

#### New Command Format:
```bash
./image_filter_parallel input.png output.png <num_threads> <filter> [params]
```

#### Testing:
```bash
# Compile
gcc-15 main_parallel.c -o image_filter_parallel -fopenmp -lm

# Test with different thread counts
./image_filter_parallel input.png out.png 1 sobel
./image_filter_parallel input.png out.png 2 sobel
./image_filter_parallel input.png out.png 4 sobel
./image_filter_parallel input.png out.png 8 sobel
```

**Expected Result:** âœ… All commands work, but no speedup yet (that's normal!)

---

## Phase 2: PARALLELIZE FUNCTIONS (Easiest â†’ Hardest)

### **STEP 2: Parallelize `to_grayscale()` - â­ EASIEST**

#### ğŸ¯ Why Easiest?
- âœ… Single loop, no nested loops
- âœ… No race conditions (each thread writes to different index)
- âœ… No reduction needed

#### ğŸ“ What to Add:

**Before this loop:**
```c
for(int i=0; i<w*h; i++)
```

**Add this directive:**
```c
#pragma omp parallel for default(none) shared(img, g, w, h, ch)
```

#### ğŸ§© Variable Analysis:

| Variable | Type | Reason |
|----------|------|--------|
| `img` | shared | Read-only input |
| `g` | shared | Each thread writes different index |
| `w, h, ch` | shared | Read-only constants |
| `i, r, g1, b` | private | Automatic with `parallel for` |

---

### **STEP 3: Parallelize `sobel()` - â­â­ MEDIUM**

#### ğŸ¯ Why Medium?
- âš ï¸ Nested loops (y, x)
- âœ… Need `collapse(2)` to parallelize both
- âœ… Still no race conditions

#### ğŸ“ What to Add:

**Before this loop:**
```c
for(int y=0; y<h; y++) {
    for(int x=0; x<w; x++) {
```

**Add this directive:**
```c
#pragma omp parallel for collapse(2) default(none) shared(g, out, w, h, ch, gx, gy)
```

#### ğŸ§© Variable Analysis:

| Variable | Type | Reason |
|----------|------|--------|
| `g` | shared | Read-only (from `to_grayscale`) |
| `out` | shared | Each thread writes different index |
| `gx, gy` | shared | Read-only arrays |
| `y, x, sx, sy, ky, kx, xx, yy, val, idx, mag, c` | private | Automatic |

#### âš ï¸ Important:
**Don't parallelize inner loops** (`ky`, `kx`, `c`) - they're part of the computation!

---

### **STEP 4: Parallelize `build_gaussian()` - â­â­â­ TWO PARTS**

#### **Part A: Kernel Generation Loop - NEEDS REDUCTION âš¡**

#### ğŸ¯ Why Needs Reduction?
- âš ï¸ Multiple threads adding to same `sum` variable
- âŒ Without reduction â†’ **RACE CONDITION!**

#### ğŸ“ What to Add:

**Before this loop:**
```c
for(int y=-half; y<=half; y++){
    for(int x=-half; x<=half; x++){
```

**Add this directive:**
```c
#pragma omp parallel for collapse(2) reduction(+:sum) default(none) shared(k, ksize, half, sigma)
```

#### ğŸ§© Variable Analysis:

| Variable | Type | Reason |
|----------|------|--------|
| `sum` | **reduction** | âš¡ All threads accumulate to it |
| `k` | shared | Each thread writes different index |
| `ksize, half, sigma` | shared | Read-only |
| `y, x, v` | private | Automatic |

#### ğŸ”‘ Key Concept:
```
reduction(+:sum) means:
1. Each thread gets private copy of sum
2. Each thread accumulates in its copy
3. At end, all copies are combined with + operator
4. Result stored in original sum variable
```

---

#### **Part B: Normalization Loop - SIMPLE**

#### ğŸ“ What to Add:

**Before this loop:**
```c
for(int i=0; i<ksize*ksize; i++)
```

**Add this directive:**
```c
#pragma omp parallel for default(none) shared(k, ksize, sum)
```

---

### **STEP 5: Parallelize `convolve_rgb()` - â­â­â­â­ MOST COMPLEX**

#### ğŸ¯ Why Most Complex?
- âš ï¸ Triple nested loops (y, x, c)
- âš ï¸ Only parallelize y and x (NOT c!)
- âš ï¸ Many variables to manage

#### ğŸ“ What to Add:

**Before this loop:**
```c
for(int y = 0; y < h; y++) {
    for(int x = 0; x < w; x++) {
```

**Add this directive:**
```c
#pragma omp parallel for collapse(2) default(none) shared(in, out, w, h, channels, kernel, ksize, half)
```

#### ğŸ§© Variable Analysis:

| Variable | Type | Reason |
|----------|------|--------|
| `in, out` | shared | Input/output arrays |
| `kernel` | shared | Read-only filter |
| `w, h, channels, ksize, half` | shared | Read-only constants |
| All other variables | private | Automatic |

#### âš ï¸ Critical:
**Don't parallelize the c, ky, kx loops!** Too small, overhead > benefit

---

## Phase 3: OPTIMIZATION (Optional)

### **STEP 6: Add Schedule Clauses**

Try different scheduling strategies and measure performance!

---

#### **Option 1: Static Scheduling (Default)** âš–ï¸

```c
schedule(static)
// or
schedule(static, chunk_size)
```

**When to use:**
- âœ… All iterations take similar time
- âœ… Predictable workload
- âœ… Lowest overhead

---

#### **Option 2: Dynamic Scheduling** ğŸ”„

```c
schedule(dynamic)
// or
schedule(dynamic, chunk_size)
```

**When to use:**
- âœ… Iterations vary in execution time
- âœ… Unpredictable workload
- âœ… Better load balancing

---

#### **Option 3: Guided Scheduling** ğŸ¯

```c
schedule(guided)
// or
schedule(guided, chunk_size)
```

**When to use:**
- âœ… Start with large chunks, decrease size
- âœ… Balances load and overhead
- âœ… Good compromise

---

#### Where to Add Scheduling:

**In `sobel()` parallel loop:**
```c
#pragma omp parallel for collapse(2) schedule(dynamic) default(none) shared(...)
```

**In `convolve_rgb()` parallel loop:**
```c
#pragma omp parallel for collapse(2) schedule(guided) default(none) shared(...)
```

---

#### ğŸ§ª Experiment Plan:

| Function | Test Schedules |
|----------|----------------|
| `sobel()` | static, dynamic, guided |
| `convolve_rgb()` | static, dynamic, guided |

**Measure which gives best performance for YOUR hardware!**

---

## ğŸ“Š Testing Strategy

### After Each Step:

#### 1ï¸âƒ£ **Compile:**
```bash
gcc-15 main_parallel.c -o image_filter_parallel -fopenmp -lm
```

#### 2ï¸âƒ£ **Run Baseline (1 thread):**
```bash
./image_filter_parallel input.png out1.png 1 sobel
```

#### 3ï¸âƒ£ **Run with Multiple Threads:**
```bash
./image_filter_parallel input.png out2.png 2 sobel
./image_filter_parallel input.png out4.png 4 sobel
./image_filter_parallel input.png out8.png 8 sobel
```

#### 4ï¸âƒ£ **Verify Output:**
- âœ… Check if output images are correct
- âœ… Compare with serial version output

#### 5ï¸âƒ£ **Compare Execution Times:**
```
1 thread:  X.XXX seconds (baseline)
2 threads: X.XXX seconds (speedup: X.Xx)
4 threads: X.XXX seconds (speedup: X.Xx)
8 threads: X.XXX seconds (speedup: X.Xx)
```

---

## âš ï¸ Common Mistakes to Avoid

### ğŸš« **Mistake 1: Parallelizing Inner Convolution Loops**
```c
// âŒ WRONG - Don't do this!
for(int y = 0; y < h; y++) {
    for(int x = 0; x < w; x++) {
        #pragma omp parallel for  // âŒ BAD!
        for(int ky = -half; ky <= half; ky++) {
```
**Why wrong:** Loops too small, overhead > benefit

---

### ğŸš« **Mistake 2: Parallelizing Channel Loop**
```c
// âŒ WRONG - Don't do this!
#pragma omp parallel for  // âŒ BAD!
for(int c = 0; c < channels; c++) {
```
**Why wrong:** Only 3 iterations (RGB), overhead >> benefit

---

### ğŸš« **Mistake 3: Forgetting Reduction**
```c
// âŒ WRONG - Race condition!
#pragma omp parallel for
for(int y=-half; y<=half; y++){
    for(int x=-half; x<=half; x++){
        sum += v;  // âŒ Multiple threads writing to sum!
    }
}

// âœ… CORRECT
#pragma omp parallel for reduction(+:sum)
```

---

### ğŸš« **Mistake 4: Not Using collapse(2)**
```c
// âš ï¸ SUBOPTIMAL - Only parallelizing outer loop
#pragma omp parallel for
for(int y=0; y<h; y++) {
    for(int x=0; x<w; x++) {

// âœ… BETTER - Parallelizing both loops
#pragma omp parallel for collapse(2)
for(int y=0; y<h; y++) {
    for(int x=0; x<w; x++) {
```

---

### ğŸš« **Mistake 5: Not Using default(none)**
```c
// âš ï¸ RISKY - Variable scoping bugs hidden
#pragma omp parallel for

// âœ… SAFE - Forces explicit variable declaration
#pragma omp parallel for default(none) shared(...) 
```

---

## ğŸ¯ Implementation Order

```mermaid
graph LR
    A[Start] --> B[STEP 1: Setup]
    B --> C[Test]
    C --> D[STEP 2: to_grayscale]
    D --> E[Test]
    E --> F[STEP 3: sobel]
    F --> G[Test]
    G --> H[STEP 4: build_gaussian]
    H --> I[Test]
    I --> J[STEP 5: convolve_rgb]
    J --> K[Test]
    K --> L[STEP 6: Optimize]
    L --> M[Done!]
```

### Recommended Flow:

1. âœ… **STEP 1** â†’ Setup Thread Count
2. âœ… **STEP 2** â†’ Parallelize `to_grayscale()` â†’ **Test**
3. âœ… **STEP 3** â†’ Parallelize `sobel()` â†’ **Test**
4. âœ… **STEP 4** â†’ Parallelize `build_gaussian()` â†’ **Test**
5. âœ… **STEP 5** â†’ Parallelize `convolve_rgb()` â†’ **Test**
6. âœ… **STEP 6** â†’ Add Scheduling â†’ **Test & Compare**

---

## ğŸ“ˆ Expected Speedup

### Theoretical Speedup (Amdahl's Law):

| Threads | Ideal Speedup | Realistic Speedup |
|---------|---------------|-------------------|
| 1 | 1.0x | 1.0x |
| 2 | 2.0x | 1.7-1.9x |
| 4 | 4.0x | 3.0-3.5x |
| 8 | 8.0x | 5.0-6.5x |

**Note:** Actual speedup depends on:
- Image size (larger = better parallelization)
- CPU architecture
- Memory bandwidth
- Cache effects

---

## ğŸ“ Key OpenMP Concepts Recap

### Directives Used:

| Directive | Purpose |
|-----------|---------|
| `#pragma omp parallel for` | Parallelize loop |
| `collapse(N)` | Parallelize N nested loops |
| `reduction(+:var)` | Safe accumulation across threads |
| `default(none)` | Force explicit variable scoping |
| `shared(...)` | Variables shared among threads |
| `schedule(...)` | Control work distribution |

---

## ğŸ Final Checklist

Before considering implementation complete:

- [ ] All 5 functions parallelized
- [ ] Tested with 1, 2, 4, 8 threads
- [ ] Output images verified correct
- [ ] Speedup measurements recorded
- [ ] Schedule optimization attempted
- [ ] No race conditions (check with larger images)
- [ ] Code compiles without warnings

---

## ğŸ‰ Success!

Once all steps are complete, you should see:
- âœ… 3-6x speedup with 8 threads
- âœ… Correct output images
- âœ… Understanding of OpenMP parallelization
- âœ… Production-ready parallel image filter!

---

# ğŸ’» Hardware Specifications

```
Total Cores:        10
â”œâ”€â”€ P-Cores:        4 @ 4.4 GHz
â””â”€â”€ E-Cores:        6 @ 2.8 GHz

Cache:              16 MB L2 (shared)
Memory:             16 GB Unified
Bandwidth:          120 GB/s
```

---

## ğŸ‘¤ Author

**saffisardar and saadali**

---